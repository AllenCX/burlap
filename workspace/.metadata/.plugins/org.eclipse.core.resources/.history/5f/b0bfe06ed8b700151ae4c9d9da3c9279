package ellisPaperCode.pfLearner;

import java.util.ArrayList;
import java.util.List;

import ellisPaperCode.propositionalFunctionToFeatures.LinearFVVFA;
import ellisPaperCode.propositionalFunctionToFeatures.PFToFeaturesGenerator;
import ellisPaperCode.taxi.TaxiDomain;
import ellisPaperCode.taxi.TaxiRandomStateGenerator;
import burlap.behavior.singleagent.EpisodeAnalysis;
import burlap.behavior.singleagent.Policy;
import burlap.behavior.singleagent.auxiliary.StateReachability;
import burlap.behavior.singleagent.learning.lspi.LSPI;
import burlap.behavior.singleagent.learning.lspi.SARSCollector;
import burlap.behavior.singleagent.learning.lspi.SARSData;
import burlap.behavior.singleagent.planning.commonpolicies.GreedyQPolicy;
import burlap.behavior.singleagent.vfa.FeatureDatabase;
import burlap.behavior.singleagent.vfa.StateToFeatureVectorGenerator;
import burlap.behavior.singleagent.vfa.ValueFunctionApproximation;
import burlap.behavior.singleagent.vfa.rbf.RBFFeatureDatabase;
import burlap.behavior.statehashing.DiscreteStateHashFactory;
import burlap.behavior.statehashing.StateHashFactory;
import burlap.oomdp.auxiliary.DomainGenerator;
import burlap.oomdp.auxiliary.StateGenerator;
import burlap.oomdp.core.Domain;
import burlap.oomdp.core.PropositionalFunction;
import burlap.oomdp.core.State;
import burlap.oomdp.core.TerminalFunction;
import burlap.oomdp.singleagent.RewardFunction;
import burlap.oomdp.singleagent.SADomain;
import burlap.oomdp.singleagent.common.UniformCostRF;

public class PFLearnerTest {
	public static void printPercentageTruePFs(List<PropositionalFunction> pfs, List<State> allStates) {
		System.out.print("Printing percentage of true of learned PFs...");
		StringBuilder toPrint = new StringBuilder();
		int numStates = allStates.size();
		for (PropositionalFunction pf : pfs) {
			double percTrue = 0.0;
			for (State s : allStates) {
				if (pf.isTrue(s, "")) {
					percTrue++;
				}
			}
			percTrue /= (float) numStates;
			toPrint.append(Double.toString(percTrue)+ ",");
			System.out.print(".");
		}
		System.out.println("");
		System.out.println(toPrint.toString());
	}
	
	
	public static void main(String[] args) {
		DomainGenerator dg = new TaxiDomain();
		Domain d = dg.generateDomain();
		State initialState = TaxiDomain.getClassicState(d);
		TerminalFunction tf = new TaxiDomain.TaxiTF();
		RewardFunction rf = new UniformCostRF();
		List<State> allStates = StateReachability.getReachableStates(initialState, (SADomain)d, new DiscreteStateHashFactory());

		List<PropositionalFunction> learnedPropFuns = null;

		//Test exhaustive learning
//		PFLearner pfLearnerEx = new PFLearner();
//		learnedPropFuns = pfLearnerEx.getPropFuns(d, rf, tf, initialState);
//		printPercentageTruePFs(learnedPropFuns, allStates);

		//Test learning using a rollout policy
		int numRollouts = 10;
		int policyRolloutLength = 100;
		PFLearner pfLearnerPol = new PFLearner(new Policy.RandomPolicy(d), policyRolloutLength, numRollouts);
		learnedPropFuns = pfLearnerPol.getPropFuns(d, rf, tf, initialState);
//		printPercentageTruePFs(learnedPropFuns, allStates);
		
		StateToFeatureVectorGenerator featGen = new PFToFeaturesGenerator(learnedPropFuns);
		ValueFunctionApproximation vfApprox = new LinearFVVFA(featGen, 0.0);
		
		FVToFeatureDatabase databaseConverter = new FVToFeatureDataBase();
		FeatureDatabase
		
//		
//		System.out.println("LearnedPFs: " + learnedPropFuns.toString());
//		
//		//Try and run LSPI with learned PFs as features
//		
//		//get a state definition earlier, we'll use it soon.
//
//		StateGenerator rStateGen = new TaxiRandomStateGenerator(d, initialState);
//		SARSCollector collector = new SARSCollector.UniformRandomSARSCollector(d);
//		SARSData dataset = collector.collectNInstances(rStateGen, rf, 5000, 100, tf, null);
//
//		//instantiate an RBF feature database, we'll define it more in a moment
//		RBFFeatureDatabase rbf = new RBFFeatureDatabase(true);
//
//
//		//notice we pass LSPI our RBF features this time
//		LSPI lspi = new LSPI(d, rf, tf, 0.99, rbf);
//		lspi.setDataset(dataset);
//
//		lspi.runPolicyIteration(30, 1e-6);
//
//		GreedyQPolicy p = new GreedyQPolicy(lspi);
//		
//		EpisodeAnalysis ea = p.evaluateBehavior(initialState, rf, 35);
//		System.out.println("ACTIONS TAKEN BY LSPI: " + ea.getActionSequenceString());

	}

}
